{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RHPmQ92sPDp",
        "outputId": "d33a90f2-c63f-4c0b-eb75-5182cc56a48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"VentasPorCiudad\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "ventas = [\n",
        "    (\"Lima\", [(\"Producto1\", 5, 10.0), (\"Producto2\", 2, 20.0)]),\n",
        "    (\"Cusco\", [(\"Producto1\", 3, 10.0), (\"Producto3\", 1, 15.0)]),\n",
        "    (\"Lima\", [(\"Producto1\", 1, 10.0), (\"Producto2\", 1, 20.0)]),\n",
        "    (\"Cusco\", [(\"Producto3\", 5, 15.0), (\"Producto2\", 1, 20.0)]),\n",
        "    (\"Arequipa\", [(\"Producto1\", 2, 10.0), (\"Producto3\", 1, 15.0)])\n",
        "]\n",
        "rdd = sc.parallelize(ventas)\n",
        "\n",
        "ventas_por_ciudad = rdd.map(lambda ciudad: (\n",
        "    ciudad[0],  # Nombre de la ciudad\n",
        "    sum([cantidad * precio for _, cantidad, precio in ciudad[1]])  # Calcular el total de ventas en esa ciudad\n",
        "))\n",
        "\n",
        "# Reducir por clave (ciudad) para obtener el total de ventas final en cada ciudad\n",
        "ventas_totales_por_ciudad = ventas_por_ciudad.reduceByKey(lambda x, y: x + y)\n",
        "print(ventas_totales_por_ciudad.collect())\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z68saFEBN9BB",
        "outputId": "4e00f383-6e35-4610-97ac-7aeb2293fa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Lima', 120.0), ('Cusco', 140.0), ('Arequipa', 35.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"FiltrarVentasPorCiudad\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "rdd = sc.parallelize(ventas)\n",
        "\n",
        "# Calcular el total de ventas por ciudad\n",
        "ventas_por_ciudad = rdd.map(lambda ciudad: (\n",
        "    ciudad[0],  # Nombre de la ciudad\n",
        "    sum([cantidad * precio for _, cantidad, precio in ciudad[1]])  # Calcular el total de ventas en esa ciudad\n",
        "))\n",
        "\n",
        "# Filtrar solo las ciudades con ventas mayores a 50\n",
        "ventas_filtradas = ventas_por_ciudad.filter(lambda x: x[1] > 50)\n",
        "print(ventas_filtradas.collect())\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6II0noGpi3l",
        "outputId": "f6bb1f52-9fcd-4ec0-df3b-2ec467a4a24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Lima', 90.0), ('Cusco', 95.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ventas = [\n",
        "    (\"Lima\", [(\"Producto1\", 5, 10.0), (\"Producto2\", 2, 20.0)]),\n",
        "    (\"Cusco\", [(\"Producto1\", 3, 10.0), (\"Producto3\", 1, 15.0)]),\n",
        "    (\"Lima\", [(\"Producto1\", 1, 10.0), (\"Producto2\", 1, 20.0)]),\n",
        "    (\"Cusco\", [(\"Producto3\", 5, 15.0), (\"Producto2\", 1, 20.0)]),\n",
        "    (\"Arequipa\", [(\"Producto1\", 2, 10.0), (\"Producto3\", 1, 15.0)])\n",
        "]\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"VentasIndividuales\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "rdd = sc.parallelize(ventas)\n",
        "ventas_individuales = rdd.flatMap(lambda ciudad: [\n",
        "    (ciudad[0], producto, cantidad, cantidad * precio)  # Calcula el total por producto\n",
        "    for producto, cantidad, precio in ciudad[1]\n",
        "])\n",
        "print(ventas_individuales.collect())\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL4MSzR_qPGR",
        "outputId": "03ba27b3-8949-4327-85a9-8a16f0363da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Lima', 'Producto1', 5, 50.0), ('Lima', 'Producto2', 2, 40.0), ('Cusco', 'Producto1', 3, 30.0), ('Cusco', 'Producto3', 1, 15.0), ('Lima', 'Producto1', 1, 10.0), ('Lima', 'Producto2', 1, 20.0), ('Cusco', 'Producto3', 5, 75.0), ('Cusco', 'Producto2', 1, 20.0), ('Arequipa', 'Producto1', 2, 20.0), ('Arequipa', 'Producto3', 1, 15.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "conf = SparkConf().setAppName(\"UnionEstudiantes\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "estudiantes_colegio_A = [\n",
        "    (\"Ana\", \"Colegio A\", 85),\n",
        "    (\"Luis\", \"Colegio A\", 78),\n",
        "    (\"Carlos\", \"Colegio A\", 92)\n",
        "]\n",
        "\n",
        "estudiantes_colegio_B = [\n",
        "    (\"Maria\", \"Colegio B\", 88),\n",
        "    (\"Jose\", \"Colegio B\", 73),\n",
        "    (\"Lucia\", \"Colegio B\", 95)\n",
        "]\n",
        "rdd_colegio_A = sc.parallelize(estudiantes_colegio_A)\n",
        "rdd_colegio_B = sc.parallelize(estudiantes_colegio_B)\n",
        "rdd_estudiantes_total = rdd_colegio_A.union(rdd_colegio_B)\n",
        "print(rdd_estudiantes_total.collect())\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alGOCezkqrPP",
        "outputId": "e622a26b-e67d-4dd1-ac98-280dec23c42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ana', 'Colegio A', 85), ('Luis', 'Colegio A', 78), ('Carlos', 'Colegio A', 92), ('Maria', 'Colegio B', 88), ('Jose', 'Colegio B', 73), ('Lucia', 'Colegio B', 95)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"IntersectionEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Datos de ejemplo: productos vendidos a los clientes\n",
        "productos_vendidos = [\n",
        "    (\"ProductoA\", 10),\n",
        "    (\"ProductoB\", 15),\n",
        "    (\"ProductoC\", 5),\n",
        "    (\"ProductoD\", 20)\n",
        "]\n",
        "\n",
        "# Datos de ejemplo: productos solicitados por los clientes\n",
        "productos_solicitados = [\n",
        "    (\"ProductoB\", 15),\n",
        "    (\"ProductoD\", 20),\n",
        "    (\"ProductoE\", 30),\n",
        "    (\"ProductoF\", 25)\n",
        "]\n",
        "rdd_vendidos = sc.parallelize(productos_vendidos)\n",
        "rdd_solicitados = sc.parallelize(productos_solicitados)\n",
        "\n",
        "# Encontrar los productos comunes entre ambos RDDs usando intersection()\n",
        "productos_comunes = rdd_vendidos.intersection(rdd_solicitados)\n",
        "print(productos_comunes.collect())\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drPHcusmrctD",
        "outputId": "cc03b32a-7014-4947-ee0b-9a490bb53668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ProductoD', 20), ('ProductoB', 15)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"DistinctEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "ventas = [\n",
        "    (\"ProductoA\", 10),\n",
        "    (\"ProductoB\", 15),\n",
        "    (\"ProductoA\", 10),\n",
        "    (\"ProductoC\", 5),\n",
        "    (\"ProductoB\", 15),\n",
        "    (\"ProductoD\", 20),\n",
        "    (\"ProductoC\", 5)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Eliminar duplicados usando distinct()\n",
        "ventas_unicas = rdd_ventas.distinct()\n",
        "print(ventas_unicas.collect())\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZx6knb4rq00",
        "outputId": "61d4a9ad-1a77-4a55-e695-9592f1479734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ProductoA', 10), ('ProductoB', 15), ('ProductoC', 5), ('ProductoD', 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setAppName(\"GroupByKeyEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "ventas = [\n",
        "    (\"Lima\", 10),\n",
        "    (\"Moyobamba\", 15),\n",
        "    (\"Lima\", 20),\n",
        "    (\"Rioja\", 5),\n",
        "    (\"Moyobamba\", 10),\n",
        "    (\"Rioja\", 7)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "ventas_por_ciudad = rdd_ventas.groupByKey()\n",
        "\n",
        "for ciudad, ventas in ventas_por_ciudad.collect():\n",
        "    print(f\"Ciudad: {ciudad}, Ventas: {list(ventas)}\")\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5fP_2dsjHHw",
        "outputId": "33592930-9b65-471a-a21c-0c0e27c0ce4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciudad: Lima, Ventas: [10, 20]\n",
            "Ciudad: Moyobamba, Ventas: [15, 10]\n",
            "Ciudad: Rioja, Ventas: [5, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"SortByKeyEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Datos de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 20)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "ventas_ordenadas = rdd_ventas.sortByKey()\n",
        "\n",
        "# Mostrar los resultados\n",
        "for ciudad, total_ventas in ventas_ordenadas.collect():\n",
        "    print(f\"Ciudad: {ciudad}, Total de Ventas: {total_ventas}\")\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNCDts4Iihks",
        "outputId": "acd89bf8-a8ab-490f-f15d-563614412f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciudad: Arequipa, Total de Ventas: 18\n",
            "Ciudad: Cusco, Total de Ventas: 20\n",
            "Ciudad: Lima, Total de Ventas: 30\n",
            "Ciudad: Moyobamba, Total de Ventas: 25\n",
            "Ciudad: Rioja, Total de Ventas: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"JoinEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 20)\n",
        "]\n",
        "poblacion = [\n",
        "    (\"Lima\", 9000000),\n",
        "    (\"Moyobamba\", 50000),\n",
        "    (\"Rioja\", 35000),\n",
        "    (\"Arequipa\", 1000000),\n",
        "    (\"Cusco\", 500000)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "rdd_poblacion = sc.parallelize(poblacion)\n",
        "\n",
        "# Realizar el join entre los RDDs por la clave (ciudad)\n",
        "rdd_join = rdd_ventas.join(rdd_poblacion)\n",
        "# Mostrar los resultados\n",
        "for ciudad, (total_ventas, poblacion_ciudad) in rdd_join.collect():\n",
        "    print(f\"Ciudad: {ciudad}, Ventas: {total_ventas}, Población: {poblacion_ciudad}\")\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aW15mtBjbqO",
        "outputId": "5755de14-91e5-4bc7-d874-5e1f0416a980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciudad: Moyobamba, Ventas: 25, Población: 50000\n",
            "Ciudad: Rioja, Ventas: 12, Población: 35000\n",
            "Ciudad: Arequipa, Ventas: 18, Población: 1000000\n",
            "Ciudad: Lima, Ventas: 30, Población: 9000000\n",
            "Ciudad: Cusco, Ventas: 20, Población: 500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setAppName(\"CogroupEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18)\n",
        "]\n",
        "salarios = [\n",
        "    (\"Lima\", 1200),\n",
        "    (\"Moyobamba\", 800),\n",
        "    (\"Cusco\", 900),\n",
        "    (\"Arequipa\", 1100)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "rdd_salarios = sc.parallelize(salarios)\n",
        "rdd_cogroup = rdd_ventas.cogroup(rdd_salarios)\n",
        "for ciudad, (ventas_ciudad, salarios_ciudad) in rdd_cogroup.collect():\n",
        "    print(f\"Ciudad: {ciudad}\")\n",
        "    print(f\"  Ventas: {list(ventas_ciudad)}\")\n",
        "    print(f\"  Salarios: {list(salarios_ciudad)}\")\n",
        "    print(\"-\" * 30)\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILqvl1gljfd6",
        "outputId": "c762491d-16e0-4d6f-e574-437c8af0b0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciudad: Moyobamba\n",
            "  Ventas: [25]\n",
            "  Salarios: [800]\n",
            "------------------------------\n",
            "Ciudad: Rioja\n",
            "  Ventas: [12]\n",
            "  Salarios: []\n",
            "------------------------------\n",
            "Ciudad: Arequipa\n",
            "  Ventas: [18]\n",
            "  Salarios: [1100]\n",
            "------------------------------\n",
            "Ciudad: Lima\n",
            "  Ventas: [30]\n",
            "  Salarios: [1200]\n",
            "------------------------------\n",
            "Ciudad: Cusco\n",
            "  Ventas: []\n",
            "  Salarios: [900]\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"CoalesceEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear los datos de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "rdd_ventas = sc.parallelize(ventas, 6)  # Utilizamos 6 particiones al principio\n",
        "\n",
        "# Mostrar el número de particiones antes de usar coalesce\n",
        "print(f\"Particiones antes de coalesce: {rdd_ventas.getNumPartitions()}\")\n",
        "\n",
        "# Reducir las particiones a 2\n",
        "rdd_coalesced = rdd_ventas.coalesce(2)\n",
        "\n",
        "# Mostrar el número de particiones después de usar coalesce\n",
        "print(f\"Particiones después de coalesce: {rdd_coalesced.getNumPartitions()}\")\n",
        "\n",
        "# Mostrar los datos después de coalesce\n",
        "print(rdd_coalesced.collect())\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE4585Myjnhf",
        "outputId": "d2113155-25c0-4797-b190-074a8ef6b653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Particiones antes de coalesce: 6\n",
            "Particiones después de coalesce: 2\n",
            "[('Lima', 30), ('Moyobamba', 25), ('Rioja', 12), ('Arequipa', 18), ('Cusco', 10), ('Iquitos', 22)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Acciones"
      ],
      "metadata": {
        "id": "HcIPGV1gjp0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"ReduceEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Usar reduce para calcular el total de ventas\n",
        "# Función de reducción que suma los valores\n",
        "total_ventas = rdd_ventas.map(lambda x: x[1]).reduce(lambda a, b: a + b)\n",
        "\n",
        "# Mostrar el total de ventas\n",
        "print(f\"Total de ventas: {total_ventas}\")\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwptvpYkHK3",
        "outputId": "7de634af-5e14-49cb-e34f-08901bd60aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de ventas: 117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"CollectEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Usar collect() para recuperar todos los elementos del RDD\n",
        "resultado = rdd_ventas.collect()\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(\"Ventas por ciudad:\", resultado)\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJAZgcOkKtg",
        "outputId": "9d4ff8f0-b51f-443e-891b-3c1ff71b7c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ventas por ciudad: [('Lima', 30), ('Moyobamba', 25), ('Rioja', 12), ('Arequipa', 18), ('Cusco', 10), ('Iquitos', 22)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"CountEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Usar count() para contar el número de elementos en el RDD\n",
        "total_elementos = rdd_ventas.count()\n",
        "\n",
        "# Mostrar el total de elementos\n",
        "print(f\"Total de elementos en el RDD: {total_elementos}\")\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC1elfMhkQW7",
        "outputId": "5836d3f7-6a4e-46ba-c377-08ef78b90a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de elementos en el RDD: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"FirstEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Usar first() para obtener el primer elemento del RDD\n",
        "primer_venta = rdd_ventas.first()\n",
        "\n",
        "# Mostrar el primer elemento\n",
        "print(f\"Primer elemento del RDD: {primer_venta}\")\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Svkqi_2kUBW",
        "outputId": "a6e6d303-b238-465f-826d-aadb88e43069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primer elemento del RDD: ('Lima', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"TakeEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear un RDD de ventas\n",
        "ventas = [\n",
        "    (\"Lima\", 30),\n",
        "    (\"Moyobamba\", 25),\n",
        "    (\"Rioja\", 12),\n",
        "    (\"Arequipa\", 18),\n",
        "    (\"Cusco\", 10),\n",
        "    (\"Iquitos\", 22)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Usar take(n) para obtener los primeros 3 elementos del RDD\n",
        "primeros_3 = rdd_ventas.take(3)\n",
        "\n",
        "# Mostrar los primeros 3 elementos\n",
        "print(f\"Primeros 3 elementos del RDD: {primeros_3}\")\n",
        "\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "Z87lMj5Qk_wH",
        "outputId": "caac90eb-acb4-4799-e63b-1d1d377ba979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeros 3 elementos del RDD: [('Lima', 30), ('Moyobamba', 25), ('Rioja', 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import shutil\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass  # Si no hay un contexto activo, se ignora el error\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"SaveAsTextFileEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear el RDD con los datos de ventas\n",
        "ventas = [\n",
        "    (\"T001\", \"Laptop\", 2),\n",
        "    (\"T002\", \"Smartphone\", 5),\n",
        "    (\"T003\", \"Tablet\", 1),\n",
        "    (\"T004\", \"Laptop\", 3),\n",
        "    (\"T005\", \"Smartphone\", 2),\n",
        "    (\"T006\", \"Smartwatch\", 4),\n",
        "    (\"T007\", \"Laptop\", 1),\n",
        "    (\"T008\", \"Smartphone\", 3)\n",
        "]\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "# Crear un RDD con (producto, cantidad) usando map\n",
        "rdd_productos = rdd_ventas.map(lambda x: (x[1], x[2]))\n",
        "# Calcular el total vendido por cada producto utilizando reduceByKey\n",
        "rdd_totales = rdd_productos.reduceByKey(lambda x, y: x + y)\n",
        "# Convertir cada tupla en una cadena de texto para guardarlas como texto\n",
        "rdd_totales_texto = rdd_totales.map(lambda x: f\"{x[0]}: {x[1]}\")\n",
        "# Eliminar el directorio de salida si ya existe\n",
        "try:\n",
        "    shutil.rmtree(\"ventas_totales_salida\")\n",
        "except FileNotFoundError:\n",
        "    pass  # Si el directorio no existe, ignora el error\n",
        "# Guardar el resultado como archivo de texto\n",
        "rdd_totales_texto.saveAsTextFile(\"ventas_totales_salida\")\n",
        "# Detener el contexto de Spark al final\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "N9QUnOWvyY-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"MaxMinEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear el RDD con los datos de ventas\n",
        "ventas = [\n",
        "    (\"T001\", \"Laptop\", 2),\n",
        "    (\"T002\", \"Smartphone\", 5),\n",
        "    (\"T003\", \"Tablet\", 1),\n",
        "    (\"T004\", \"Laptop\", 3),\n",
        "    (\"T005\", \"Smartphone\", 2),\n",
        "    (\"T006\", \"Smartwatch\", 4),\n",
        "    (\"T007\", \"Laptop\", 1),\n",
        "    (\"T008\", \"Smartphone\", 3)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Crear un RDD con (producto, cantidad) usando map\n",
        "rdd_productos = rdd_ventas.map(lambda x: (x[1], x[2]))\n",
        "\n",
        "# Encontrar el producto con la venta más alta (max) y la más baja (min)\n",
        "max_venta = rdd_productos.reduceByKey(lambda x, y: x + y).max(lambda x: x[1])\n",
        "min_venta = rdd_productos.reduceByKey(lambda x, y: x + y).min(lambda x: x[1])\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Producto con la venta más alta: {max_venta}\")\n",
        "print(f\"Producto con la venta más baja: {min_venta}\")\n",
        "\n",
        "# Detener el contexto de Spark\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "NtcS8ild0IzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e9437b-e6e1-432c-aaeb-8d1b262d64ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producto con la venta más alta: ('Smartphone', 10)\n",
            "Producto con la venta más baja: ('Tablet', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"CountByKeyEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear el RDD con los datos de ventas\n",
        "ventas = [\n",
        "    (\"Laptop\", 2),\n",
        "    (\"Smartphone\", 5),\n",
        "    (\"Tablet\", 1),\n",
        "    (\"Laptop\", 3),\n",
        "    (\"Smartphone\", 2),\n",
        "    (\"Smartwatch\", 4),\n",
        "    (\"Laptop\", 1),\n",
        "    (\"Smartphone\", 3)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Contar cuántas veces se vendió cada producto usando countByKey\n",
        "conteo_por_producto = rdd_ventas.countByKey()\n",
        "\n",
        "# Imprimir los resultados\n",
        "for producto, conteo in conteo_por_producto.items():\n",
        "    print(f\"{producto}: {conteo}\")\n",
        "\n",
        "# Detener el contexto de Spark\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odnu5g1K0L7O",
        "outputId": "f63abbc0-b749-4669-e07b-8ebf1035afef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laptop: 3\n",
            "Smartphone: 3\n",
            "Tablet: 1\n",
            "Smartwatch: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configurar y crear el contexto de Spark\n",
        "conf = SparkConf().setAppName(\"ForEachEjemplo\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear el RDD con los datos de ventas\n",
        "ventas = [\n",
        "    (\"Laptop\", 2),\n",
        "    (\"Smartphone\", 5),\n",
        "    (\"Tablet\", 1),\n",
        "    (\"Laptop\", 3),\n",
        "    (\"Smartphone\", 2),\n",
        "    (\"Smartwatch\", 4),\n",
        "    (\"Laptop\", 1),\n",
        "    (\"Smartphone\", 3)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "# Función que imprimirá cada venta\n",
        "def imprimir_venta(venta):\n",
        "    producto, cantidad = venta\n",
        "    print(f\"Producto: {producto}, Cantidad vendida: {cantidad}\")\n",
        "\n",
        "# Aplicar foreach para imprimir cada venta\n",
        "rdd_ventas.foreach(imprimir_venta)\n",
        "\n",
        "# Detener el contexto de Spark\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "awsWpm1G0Sc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}